# ğŸ¤Ÿ Sign Language Gesture Detection with YOLOv8

This project uses a custom-trained YOLOv8 model to detect and classify hand gestures representing sign language in real time through your webcam.

## ğŸ“¸ Demo
![demo](docs/demo.gif) <!-- Optional if you add a GIF -->

## ğŸ§  Project Highlights

- Trained on 3 custom hand gestures like:
  - `loveyou`
  - `yes`
  - `stop`
  

- Real-time gesture detection using OpenCV
- Built with Ultralytics' YOLOv8
- Custom dataset collected and annotated using Roboflow

---

## ğŸ“ Folder Structure

```
sign-language-detection/
â”œâ”€â”€ roboflow/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ valid/
â”‚   â””â”€â”€ data.yaml                # Trained YOLOv8 model weights
â”œâ”€â”€ capture.py      # Capture and save labeled hand gesture images
â”œâ”€â”€ train_mode.py         # YOLOv8 model training script
â”œâ”€â”€ detection.py           # Real-time webcam detection
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENCSE
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt

```
Create and Setup Environment
Select Python interpretor as Environment(Press F1)
Activate Environment
```bash
pip install -r requirements.txt

if you want to use GPU then install this pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118



### 2ï¸âƒ£ Prepare Dataset

You can:
- Collect your own using `capture.py`
- and label using robflow or Download pre Labelled dataset and push it in robflow folder

Make sure your folder structure looks like this:
```
roboflow/
â”œâ”€â”€ train/
â”œâ”€â”€ valid/
â””â”€â”€ data.yaml
```

### 3ï¸âƒ£ Train the Model

```bash
python train_mode.py
```

### 4ï¸âƒ£ Run Real-Time Detection

```bash
python detection.py
```

---

## ğŸ§  Training Config (YOLOv8)

- `imgsz`: 1280
- `epochs`: 100
- `batch`: 8
- `model`: yolov8n.pt or yolov8s.pt

---

## ğŸ¤ Contributing

Pull requests and forks are welcome! Add more gestures or improve accuracy.

---

## ğŸ“œ License

This project is open source under the MIT License.
